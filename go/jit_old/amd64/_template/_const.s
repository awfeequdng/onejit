	.file	"const.s"
	.text

	.p2align 4,,15
	.globl	Const_s8
	.type	Const_s8, @function
Const_s8:
	.cfi_startproc
	add	$0x2f,%rax
	add	$0x2f,%rcx
	add	$0x2f,%rdx
	add	$0x2f,%rbx
	add	$0x2f,%rsp
	add	$0x2f,%rbp
	add	$0x2f,%rsi
	add	$0x2f,%rdi
	add	$0x2f,%r8
	add	$0x2f,%r9
	add	$0x2f,%r10
	add	$0x2f,%r11
	add	$0x2f,%r12
	add	$0x2f,%r13
	add	$0x2f,%r14
	add	$0x2f,%r15
        nop
	or 	$0x2f,%rax
	or 	$0x2f,%rcx
	or 	$0x2f,%rdx
	or 	$0x2f,%rbx
	or 	$0x2f,%rsp
	or 	$0x2f,%rbp
	or 	$0x2f,%rsi
	or 	$0x2f,%rdi
	or 	$0x2f,%r8
	or 	$0x2f,%r9
	or 	$0x2f,%r10
	or 	$0x2f,%r11
	or      $0x2f,%r12
	or 	$0x2f,%r13
	or 	$0x2f,%r14
	or 	$0x2f,%r15
        nop
	and	$0x2f,%rax
	and	$0x2f,%rcx
	and	$0x2f,%rdx
	and	$0x2f,%rbx
	and	$0x2f,%rsp
	and	$0x2f,%rbp
	and	$0x2f,%rsi
	and	$0x2f,%rdi
	and	$0x2f,%r8
	and	$0x2f,%r9
	and	$0x2f,%r10
	and	$0x2f,%r11
	and	$0x2f,%r12
	and	$0x2f,%r13
	and	$0x2f,%r14
	and	$0x2f,%r15
        nop
	sub	$0x2f,%rax
	sub	$0x2f,%rcx
	sub	$0x2f,%rdx
	sub	$0x2f,%rbx
	sub	$0x2f,%rsp
	sub	$0x2f,%rbp
	sub	$0x2f,%rsi
	sub	$0x2f,%rdi
	sub	$0x2f,%r8
	sub	$0x2f,%r9
	sub	$0x2f,%r10
	sub	$0x2f,%r11
	sub	$0x2f,%r12
	sub	$0x2f,%r13
	sub	$0x2f,%r14
	sub	$0x2f,%r15
        nop
	xor	$0x2f,%rax
	xor	$0x2f,%rcx
	xor	$0x2f,%rdx
	xor	$0x2f,%rbx
	xor	$0x2f,%rsp
	xor	$0x2f,%rbp
	xor	$0x2f,%rsi
	xor	$0x2f,%rdi
	xor	$0x2f,%r8
	xor	$0x2f,%r9
	xor	$0x2f,%r10
	xor	$0x2f,%r11
	xor	$0x2f,%r12
	xor	$0x2f,%r13
	xor	$0x2f,%r14
	xor	$0x2f,%r15
        nop
	mov	$0x2f,%rax
	mov	$0x2f,%rcx
	mov	$0x2f,%rdx
	mov	$0x2f,%rbx
	mov	$0x2f,%rsp
	mov	$0x2f,%rbp
	mov	$0x2f,%rsi
	mov	$0x2f,%rdi
	mov	$0x2f,%r8
	mov	$0x2f,%r9
	mov	$0x2f,%r10
	mov	$0x2f,%r11
	mov	$0x2f,%r12
	mov	$0x2f,%r13
	mov	$0x2f,%r14
	mov	$0x2f,%r15
        nop
        ret
	.cfi_endproc


	.p2align 4,,15
	.globl	Const_u8
	.type	Const_u8, @function
Const_u8:
	.cfi_startproc
	add	$0x80,%rax
	add	$0x80,%rcx
	add	$0x80,%rdx
	add	$0x80,%rbx
	add	$0x80,%rsp
	add	$0x80,%rbp
	add	$0x80,%rsi
	add	$0x80,%rdi
	add	$0x80,%r8
	add	$0x80,%r9
	add	$0x80,%r10
	add	$0x80,%r11
	add	$0x80,%r12
	add	$0x80,%r13
	add	$0x80,%r14
	add	$0x80,%r15
        nop
	or 	$0x80,%rax
	or 	$0x80,%rcx
	or 	$0x80,%rdx
	or 	$0x80,%rbx
	or 	$0x80,%rsp
	or 	$0x80,%rbp
	or 	$0x80,%rsi
	or 	$0x80,%rdi
	or 	$0x80,%r8
	or 	$0x80,%r9
	or 	$0x80,%r10
	or 	$0x80,%r11
	or      $0x80,%r12
	or 	$0x80,%r13
	or 	$0x80,%r14
	or 	$0x80,%r15
        nop
	and	$0x80,%rax
	and	$0x80,%rcx
	and	$0x80,%rdx
	and	$0x80,%rbx
	and	$0x80,%rsp
	and	$0x80,%rbp
	and	$0x80,%rsi
	and	$0x80,%rdi
	and	$0x80,%r8
	and	$0x80,%r9
	and	$0x80,%r10
	and	$0x80,%r11
	and	$0x80,%r12
	and	$0x80,%r13
	and	$0x80,%r14
	and	$0x80,%r15
        nop
	sub	$0x80,%rax
	sub	$0x80,%rcx
	sub	$0x80,%rdx
	sub	$0x80,%rbx
	sub	$0x80,%rsp
	sub	$0x80,%rbp
	sub	$0x80,%rsi
	sub	$0x80,%rdi
	sub	$0x80,%r8
	sub	$0x80,%r9
	sub	$0x80,%r10
	sub	$0x80,%r11
	sub	$0x80,%r12
	sub	$0x80,%r13
	sub	$0x80,%r14
	sub	$0x80,%r15
        nop
	xor	$0x80,%rax
	xor	$0x80,%rcx
	xor	$0x80,%rdx
	xor	$0x80,%rbx
	xor	$0x80,%rsp
	xor	$0x80,%rbp
	xor	$0x80,%rsi
	xor	$0x80,%rdi
	xor	$0x80,%r8
	xor	$0x80,%r9
	xor	$0x80,%r10
	xor	$0x80,%r11
	xor	$0x80,%r12
	xor	$0x80,%r13
	xor	$0x80,%r14
	xor	$0x80,%r15
        nop
	mov	$0x80,%rax
	mov	$0x80,%rcx
	mov	$0x80,%rdx
	mov	$0x80,%rbx
	mov	$0x80,%rsp
	mov	$0x80,%rbp
	mov	$0x80,%rsi
	mov	$0x80,%rdi
	mov	$0x80,%r8
	mov	$0x80,%r9
	mov	$0x80,%r10
	mov	$0x80,%r11
	mov	$0x80,%r12
	mov	$0x80,%r13
	mov	$0x80,%r14
	mov	$0x80,%r15
        nop
        ret
	.cfi_endproc



	.p2align 4,,15
	.globl	Const_s32
	.type	Const_s32, @function
Const_s32:
	.cfi_startproc
	add	$0x12345678,%rax
	add	$0x12345678,%rcx
	add	$0x12345678,%rdx
	add	$0x12345678,%rbx
	add	$0x12345678,%rsp
	add	$0x12345678,%rbp
	add	$0x12345678,%rsi
	add	$0x12345678,%rdi
	add	$0x12345678,%r8
	add	$0x12345678,%r9
	add	$0x12345678,%r10
	add	$0x12345678,%r11
	add	$0x12345678,%r12
	add	$0x12345678,%r13
	add	$0x12345678,%r14
	add	$0x12345678,%r15
        nop
	or 	$0x12345678,%rax
	or 	$0x12345678,%rcx
	or 	$0x12345678,%rdx
	or 	$0x12345678,%rbx
	or 	$0x12345678,%rsp
	or 	$0x12345678,%rbp
	or 	$0x12345678,%rsi
	or 	$0x12345678,%rdi
	or 	$0x12345678,%r8
	or 	$0x12345678,%r9
	or 	$0x12345678,%r10
	or 	$0x12345678,%r11
	or      $0x12345678,%r12
	or 	$0x12345678,%r13
	or 	$0x12345678,%r14
	or 	$0x12345678,%r15
        nop
	and	$0x12345678,%rax
	and	$0x12345678,%rcx
	and	$0x12345678,%rdx
	and	$0x12345678,%rbx
	and	$0x12345678,%rsp
	and	$0x12345678,%rbp
	and	$0x12345678,%rsi
	and	$0x12345678,%rdi
	and	$0x12345678,%r8
	and	$0x12345678,%r9
	and	$0x12345678,%r10
	and	$0x12345678,%r11
	and	$0x12345678,%r12
	and	$0x12345678,%r13
	and	$0x12345678,%r14
	and	$0x12345678,%r15
        nop
	sub	$0x12345678,%rax
	sub	$0x12345678,%rcx
	sub	$0x12345678,%rdx
	sub	$0x12345678,%rbx
	sub	$0x12345678,%rsp
	sub	$0x12345678,%rbp
	sub	$0x12345678,%rsi
	sub	$0x12345678,%rdi
	sub	$0x12345678,%r8
	sub	$0x12345678,%r9
	sub	$0x12345678,%r10
	sub	$0x12345678,%r11
	sub	$0x12345678,%r12
	sub	$0x12345678,%r13
	sub	$0x12345678,%r14
	sub	$0x12345678,%r15
        nop
	xor	$0x12345678,%rax
	xor	$0x12345678,%rcx
	xor	$0x12345678,%rdx
	xor	$0x12345678,%rbx
	xor	$0x12345678,%rsp
	xor	$0x12345678,%rbp
	xor	$0x12345678,%rsi
	xor	$0x12345678,%rdi
	xor	$0x12345678,%r8
	xor	$0x12345678,%r9
	xor	$0x12345678,%r10
	xor	$0x12345678,%r11
	xor	$0x12345678,%r12
	xor	$0x12345678,%r13
	xor	$0x12345678,%r14
	xor	$0x12345678,%r15
        nop
	mov	$0x12345678,%rax
	mov	$0x12345678,%rcx
	mov	$0x12345678,%rdx
	mov	$0x12345678,%rbx
	mov	$0x12345678,%rsp
	mov	$0x12345678,%rbp
	mov	$0x12345678,%rsi
	mov	$0x12345678,%rdi
	mov	$0x12345678,%r8
	mov	$0x12345678,%r9
	mov	$0x12345678,%r10
	mov	$0x12345678,%r11
	mov	$0x12345678,%r12
	mov	$0x12345678,%r13
	mov	$0x12345678,%r14
	mov	$0x12345678,%r15
        nop
        ret
	.cfi_endproc

	.p2align 4,,15
	.globl	Const_u64
	.type	Const_u64, @function
Const_u64:
	.cfi_startproc
	movabs	$0x1234567890ABCDEF,%rax
	movabs	$0x1234567890ABCDEF,%rcx
	movabs	$0x1234567890ABCDEF,%rdx
	movabs	$0x1234567890ABCDEF,%rbx
	movabs	$0x1234567890ABCDEF,%rsp
	movabs	$0x1234567890ABCDEF,%rbp
	movabs	$0x1234567890ABCDEF,%rsi
	movabs	$0x1234567890ABCDEF,%rdi
	movabs	$0x1234567890ABCDEF,%r8
	movabs	$0x1234567890ABCDEF,%r9
	movabs	$0x1234567890ABCDEF,%r10
	movabs	$0x1234567890ABCDEF,%r11
	movabs	$0x1234567890ABCDEF,%r12
	movabs	$0x1234567890ABCDEF,%r13
	movabs	$0x1234567890ABCDEF,%r14
	movabs	$0x1234567890ABCDEF,%r15
        ret
	.cfi_endproc

